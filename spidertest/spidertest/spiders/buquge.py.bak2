# -*- coding: utf-8 -*-
import scrapy
import requests
from bs4 import BeautifulSoup
import re
from ..items import SpidertestItem
from ..items import ChapterItem
import time
import os

class BuqugeSpider(scrapy.Spider):
    name = 'buquge'
    allowed_domains = ['biquge5.com']
    start_urls = ['http://www.biquge5.com/xiaoshuodaquan']

    def parse(self, response):
        # print(type(response.text))
        # time.sleep(0.5)
        # text=response.xpath('//div[@class="novellist"]/ul/li/a').extract()
        # text=bs.find_all('li')
        # text=re.findall('<a (.*?)>"(.*?)"<em>(.*?)</em>',response.text,re.S)
        text=re.findall('<a href="(.*?)">(.*?) <em>(.*?)</em>',response.text)
        # for i in text:
        #     # print(type(i))
        #     item = SpidertestItem()
        #     item['href']='http://www.biquge5.com'+i[0]
        #     item['name']=i[1]
        #     item['author']=i[2]
        #     # print(item['author'],item['name'],'******************')
        #     yield item
        # print(1111115556666622222222)
        for i in text:
            item = SpidertestItem()
            item['href']='http://www.biquge5.com'+i[0]
            item['name']=i[1]
            item['author']=i[2]
            isExists = os.path.exists("../novel/"+i[1])
            if not isExists:
                os.makedirs('../novel/'+i[1])
            yield scrapy.Request(url='http://www.biquge5.com'+i[0],callback=self.parse_content)
#小说
    def parse_content(self,response):
        time.sleep(0.5)
        novel_name=response.xpath('//*[@id="info"]/h1/text()').extract_first()
        jieshao=response.xpath('//*[@id="intro"]/p/text()')
        text=re.findall('<ul class="_chapter">(.*?)</ul>',response.text,re.S)
        chapters = re.findall('<a href="(.*?)\r\n\t\t\t(.*?)">(.*?)</a>', text[0],re.S)
        for item in chapters:
            chapter=ChapterItem()
            chapter['name']=novel_name
            chapter['href']=item[0]+item[1]
            chapter['chapterName']=item[2]
            isExists = os.path.exists("../novel/" + novel_name)
            if not isExists:

                os.makedirs('../novel/' + novel_name)
            yield chapter
        for item in chapters:
            yield scrapy.Request(url=item[0]+item[1],callback=self.parse_chapterContent)
#章节内容提取
    def parse_chapterContent(self,response):
        time.sleep(0.5)
        novel_name=response.xpath('//*[@id="wrapper"]/div[5]/div[1]/a[3]/text()').extract_first()
        print(novel_name,'------------->>>')
        chapter_name=response.css('#wrapper h1::text').extract_first()
        print(chapter_name)
        print(type(chapter_name))
        print('/' in chapter_name)
        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&')
        if '/' in chapter_name:
            print('@@@@@@@@@@############@@@@@@@@@')
            chapter_name = re.split('（', chapter_name)
            print(chapter_name[0])
            chapter_name=chapter_name[0]

        bs=BeautifulSoup(response.text,'lxml')
        content = bs.select("#content")
        #章节内容
        print(content[0].get_text(),'+++++++++++++++++++',type(content[0].get_text()),'>>>>>>>>>>>>>>>>>')
        with open('../novel/' + novel_name+'/'+chapter_name+'.txt','a+',encoding='utf-8') as f:
            # f.write('wwwwwwww')
            f.write(content[0].get_text())

        print(response.url,"------url")
        print(type(response.url))
        # url_first=re.findall('http://(.*?)/(.*?)/(.*?).html',response.url)
        url_first=re.findall('http://(.*?).html',response.url)
        urls=re.split('/',url_first[0])
        # print(urls)
        print('http://'+urls[0]+'/'+urls[1]+'/'+urls[2]+'.html')

        url=re.findall('<a href="(.*?).html">下一章</a>',response.text)
        print(url[0])
        if '_' in url[0]:
            print('_' in url[0],url[0])
            yield scrapy.Request(url='http://'+urls[0]+'/'+urls[1]+'/'+url[0]+'.html', callback=self.parse_chapterContent)

        # print(url)
    # def parse_chapterContent2(self,response):
    #
    #     print(response.text)